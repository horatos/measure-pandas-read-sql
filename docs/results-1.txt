#1 [internal] load build definition from Dockerfile
#1 sha256:61da0256c8c6cf5455256867b35117604dffe383d09005e91803f6620a563a8c
#1 transferring dockerfile: 32B done
#1 DONE 0.0s

#2 [internal] load .dockerignore
#2 sha256:8b9a62fb1bc4e9e404f35af79ed7c23526fe3f751de52e3dfa97a492208f28b1
#2 transferring context: 2B done
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/python:3.10
#3 sha256:c787ec5cc33e1cbee663ba2529b5d51f0293f2c277b40d9bd37129383a68d5ac
#3 DONE 2.2s

#9 [1/5] FROM docker.io/library/python:3.10@sha256:95e12432e23d2116270e70e30805a057fcde85ef0fa6e6532f809478f616ace4
#9 sha256:2bbdfe68081dd02e25da8dc7d8a38c51077e414a05cae2a6a5e0124709a28c8b
#9 resolve docker.io/library/python:3.10@sha256:95e12432e23d2116270e70e30805a057fcde85ef0fa6e6532f809478f616ace4 0.0s done
#9 DONE 0.0s

#8 [internal] load build context
#8 sha256:10c9ee97e07e38e601f0e9814d3b16834d589a56efaee6d68816593f62df3c93
#8 transferring context: 4.29kB 0.0s done
#8 DONE 0.0s

#4 [2/5] WORKDIR /usr/src/app
#4 sha256:8632f694647ab185728f431f981bc2e462d7d1a8c2ffe0cfdfd7370c98687155
#4 CACHED

#5 [3/5] COPY requirements.txt ./
#5 sha256:8e6dd16aff673bf94dbff6bb9dd03083a52aa8e68893838578399fed62812caa
#5 CACHED

#6 [4/5] RUN pip install --no-cache-dir -r requirements.txt
#6 sha256:9bef38ef58a565538f674ffde2792e4d269f75d97ea9816d9a4adb832374cd14
#6 CACHED

#7 [5/5] COPY . .
#7 sha256:8f40c6ce436512442806e9c6ecaadceb31620f900b9b04f270707fe8273b53f2
#7 CACHED

#10 exporting to image
#10 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00
#10 exporting layers done
#10 writing image sha256:5110ae23fbdbc59d12a7909882cb060c9e3d7ed5a0f4321dc56e02eccff32f46 done
#10 naming to docker.io/library/measure-pandas-read-sql_app done
#10 DONE 0.0s
Network measure-pandas-read-sql_default  Creating
Network measure-pandas-read-sql_default  Created
Container measure-pandas-read-sql-postgres-1  Creating
Container measure-pandas-read-sql-mysql57-1  Creating
Container measure-pandas-read-sql-mysql8-1  Creating
Container measure-pandas-read-sql-mysql57-1  Created
Container measure-pandas-read-sql-mysql8-1  Created
Container measure-pandas-read-sql-postgres-1  Created
Container measure-pandas-read-sql-mysql8-1  Starting
Container measure-pandas-read-sql-postgres-1  Starting
Container measure-pandas-read-sql-mysql57-1  Starting
Container measure-pandas-read-sql-postgres-1  Started
Container measure-pandas-read-sql-mysql8-1  Started
Container measure-pandas-read-sql-mysql57-1  Started
[INFO] init: Start waiting databases initialization
[INFO] init: Done waiting databases initialization
[INFO] initialize_mysql8: Start adding 100000 rows
[INFO] initialize_mysql8: Added rows count = 100000
[INFO] initialize_mysql57: Start adding 100000 rows
[INFO] initialize_mysql57: Added rows count = 100000
[INFO] initialize_postgres: Start adding 100000 rows
[INFO] initialize_postgres: Added rows count = 100000
[INFO] exec_experiment_1: Execute the experiment#1 with db = mysql8
[INFO] exec_experiment_1: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    12     74.7 MiB     74.7 MiB           1   @profile
    13                                         def exec_experiment_1(db: str):
    14     74.7 MiB      0.0 MiB           1       logger.info("Execute the experiment#1 with db = %s", db)
    15                                         
    16     77.5 MiB      2.8 MiB           1       conn = create_db_engine(db).connect()
    17    123.4 MiB     45.9 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    18                                         
    19    123.4 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_2: Execute the experiment#2 with db = mysql8, chunksize = 1000
[INFO] exec_experiment_2: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    22     75.0 MiB     75.0 MiB           1   @profile
    23                                         def exec_experiment_2(db: str, chunksize: int):
    24     75.0 MiB      0.0 MiB           1       logger.info("Execute the experiment#2 with db = %s, chunksize = %s", db, chunksize)
    25     75.0 MiB      0.0 MiB           1       total = 0
    26     75.0 MiB      0.0 MiB           1       chunksize = int(chunksize)
    27                                         
    28     77.9 MiB      2.9 MiB           1       conn = create_db_engine(db).connect()
    29    118.0 MiB     40.1 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    30    118.5 MiB      0.5 MiB         101       for chunk in it:
    31    118.5 MiB      0.0 MiB         100           total += len(chunk)
    32                                         
    33    118.5 MiB      0.0 MiB           1       logger.info("Got %s records", total)


[INFO] exec_experiment_3: Execute the experiment#3 with db = mysql8
[INFO] exec_experiment_3: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    36     74.8 MiB     74.8 MiB           1   @profile
    37                                         def exec_experiment_3(db: str):
    38     74.8 MiB      0.0 MiB           1       logger.info("Execute the experiment#3 with db = %s", db)
    39                                         
    40     77.7 MiB      2.9 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    41    121.8 MiB     44.1 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    42                                         
    43    121.8 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_4: Execute the experiment#4 with db = mysql8, chunksize = 1000
[INFO] exec_experiment_4: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    46     75.0 MiB     75.0 MiB           1   @profile
    47                                         def exec_experiment_4(db: str, chunksize: int):
    48     75.0 MiB      0.0 MiB           1       logger.info("Execute the experiment#4 with db = %s, chunksize = %s", db, chunksize)
    49     75.0 MiB      0.0 MiB           1       total = 0
    50     75.0 MiB      0.0 MiB           1       chunksize = int(chunksize)
    51                                         
    52     77.8 MiB      2.8 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    53     78.2 MiB      0.4 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    54     80.3 MiB      2.1 MiB         101       for chunk in it:
    55     80.3 MiB      0.0 MiB         100           total += len(chunk)
    56                                         
    57     80.3 MiB      0.0 MiB           1       logger.info("Got %s records", total)


[INFO] exec_experiment_1: Execute the experiment#1 with db = mysql57
[INFO] exec_experiment_1: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    12     75.0 MiB     75.0 MiB           1   @profile
    13                                         def exec_experiment_1(db: str):
    14     75.0 MiB      0.0 MiB           1       logger.info("Execute the experiment#1 with db = %s", db)
    15                                         
    16     77.8 MiB      2.8 MiB           1       conn = create_db_engine(db).connect()
    17    123.8 MiB     46.0 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    18                                         
    19    123.8 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_2: Execute the experiment#2 with db = mysql57, chunksize = 1000
[INFO] exec_experiment_2: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    22     74.8 MiB     74.8 MiB           1   @profile
    23                                         def exec_experiment_2(db: str, chunksize: int):
    24     74.8 MiB      0.0 MiB           1       logger.info("Execute the experiment#2 with db = %s, chunksize = %s", db, chunksize)
    25     74.8 MiB      0.0 MiB           1       total = 0
    26     74.8 MiB      0.0 MiB           1       chunksize = int(chunksize)
    27                                         
    28     77.6 MiB      2.8 MiB           1       conn = create_db_engine(db).connect()
    29    117.7 MiB     40.1 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    30    118.2 MiB      0.5 MiB         101       for chunk in it:
    31    118.2 MiB      0.0 MiB         100           total += len(chunk)
    32                                         
    33    118.2 MiB      0.0 MiB           1       logger.info("Got %s records", total)


[INFO] exec_experiment_3: Execute the experiment#3 with db = mysql57
[INFO] exec_experiment_3: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    36     75.0 MiB     75.0 MiB           1   @profile
    37                                         def exec_experiment_3(db: str):
    38     75.0 MiB      0.0 MiB           1       logger.info("Execute the experiment#3 with db = %s", db)
    39                                         
    40     77.8 MiB      2.9 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    41    121.1 MiB     43.3 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    42                                         
    43    121.1 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_4: Execute the experiment#4 with db = mysql57, chunksize = 1000
[INFO] exec_experiment_4: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    46     74.8 MiB     74.8 MiB           1   @profile
    47                                         def exec_experiment_4(db: str, chunksize: int):
    48     74.8 MiB      0.0 MiB           1       logger.info("Execute the experiment#4 with db = %s, chunksize = %s", db, chunksize)
    49     74.8 MiB      0.0 MiB           1       total = 0
    50     74.8 MiB      0.0 MiB           1       chunksize = int(chunksize)
    51                                         
    52     77.6 MiB      2.8 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    53     78.0 MiB      0.4 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    54     80.0 MiB      2.0 MiB         101       for chunk in it:
    55     80.0 MiB      0.0 MiB         100           total += len(chunk)
    56                                         
    57     80.0 MiB      0.0 MiB           1       logger.info("Got %s records", total)


[INFO] exec_experiment_1: Execute the experiment#1 with db = postgres
[INFO] exec_experiment_1: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    12     74.7 MiB     74.7 MiB           1   @profile
    13                                         def exec_experiment_1(db: str):
    14     74.7 MiB      0.0 MiB           1       logger.info("Execute the experiment#1 with db = %s", db)
    15                                         
    16     79.0 MiB      4.3 MiB           1       conn = create_db_engine(db).connect()
    17    149.9 MiB     70.9 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    18                                         
    19    149.9 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_2: Execute the experiment#2 with db = postgres, chunksize = 1000
[INFO] exec_experiment_2: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    22     74.6 MiB     74.6 MiB           1   @profile
    23                                         def exec_experiment_2(db: str, chunksize: int):
    24     74.6 MiB      0.0 MiB           1       logger.info("Execute the experiment#2 with db = %s, chunksize = %s", db, chunksize)
    25     74.6 MiB      0.0 MiB           1       total = 0
    26     74.6 MiB      0.0 MiB           1       chunksize = int(chunksize)
    27                                         
    28     79.0 MiB      4.4 MiB           1       conn = create_db_engine(db).connect()
    29    108.4 MiB     29.4 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    30    110.3 MiB      1.3 MiB         101       for chunk in it:
    31    110.3 MiB      0.0 MiB         100           total += len(chunk)
    32                                         
    33    109.8 MiB     -0.5 MiB           1       logger.info("Got %s records", total)


[INFO] exec_experiment_3: Execute the experiment#3 with db = postgres
[INFO] exec_experiment_3: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    36     74.9 MiB     74.9 MiB           1   @profile
    37                                         def exec_experiment_3(db: str):
    38     74.9 MiB      0.0 MiB           1       logger.info("Execute the experiment#3 with db = %s", db)
    39                                         
    40     79.2 MiB      4.3 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    41    150.6 MiB     71.4 MiB           1       dataframe = pd.read_sql("SELECT * FROM users", conn)
    42                                         
    43    150.6 MiB      0.0 MiB           1       logger.info("Got %s records", len(dataframe))


[INFO] exec_experiment_4: Execute the experiment#4 with db = postgres, chunksize = 1000
[INFO] exec_experiment_4: Got 100000 records
Filename: /usr/src/app/experiments.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    46     75.0 MiB     75.0 MiB           1   @profile
    47                                         def exec_experiment_4(db: str, chunksize: int):
    48     75.0 MiB      0.0 MiB           1       logger.info("Execute the experiment#4 with db = %s, chunksize = %s", db, chunksize)
    49     75.0 MiB      0.0 MiB           1       total = 0
    50     75.0 MiB      0.0 MiB           1       chunksize = int(chunksize)
    51                                         
    52     79.3 MiB      4.3 MiB           1       conn = create_db_engine(db).connect().execution_options(stream_results=True)
    53     80.9 MiB      1.6 MiB           1       it = pd.read_sql("SELECT * FROM users", conn, chunksize=chunksize)
    54     82.9 MiB      2.0 MiB         101       for chunk in it:
    55     82.9 MiB      0.0 MiB         100           total += len(chunk)
    56                                         
    57     82.9 MiB      0.0 MiB           1       logger.info("Got %s records", total)


Container measure-pandas-read-sql-mysql57-1  Stopping
Container measure-pandas-read-sql-postgres-1  Stopping
Container measure-pandas-read-sql-postgres-1  Stopping
Container measure-pandas-read-sql-mysql8-1  Stopping
Container measure-pandas-read-sql-mysql8-1  Stopping
Container measure-pandas-read-sql-mysql57-1  Stopping
Container measure-pandas-read-sql-postgres-1  Stopped
Container measure-pandas-read-sql-postgres-1  Removing
Container measure-pandas-read-sql-postgres-1  Removed
Container measure-pandas-read-sql-mysql8-1  Stopped
Container measure-pandas-read-sql-mysql8-1  Removing
Container measure-pandas-read-sql-mysql8-1  Removed
Container measure-pandas-read-sql-mysql57-1  Stopped
Container measure-pandas-read-sql-mysql57-1  Removing
Container measure-pandas-read-sql-mysql57-1  Removed
Network measure-pandas-read-sql_default  Removing
Network measure-pandas-read-sql_default  Removed
